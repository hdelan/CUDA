\documentclass[a4paper, fleqn]{article}

\date{\today}
\author{Hugh Delaney}
\title{CUDA \\ Assignment 1}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools, geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage{xcolor}
\usepackage{listings}

\DeclarePairedDelimiterX{\inp}[2]{\langle}{\rangle}{#1, #2}

\setlength{\mathindent}{1cm}

% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{subfigure, pdfpages}
\usepackage{transparent}
\newcommand{\incfig}[1]{%
        \def\svgwidth{\columnwidth}
        \import{./figures/}{#1.pdf_tex}
}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}



\pdfsuppresswarningpagegroup=1

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}{Lemma}[theorem]

\renewcommand\qedsymbol{$\blacksquare$}

\begin{document}
\maketitle
        \section{CPU Calculation}%
        Please see \texttt{cpu\_funcs.cu} for implementation. \texttt{./matrix} accepts parameters:
        \begin{itemize}
                \item \texttt{-n [num\_rows]} specify number of rows
                \item \texttt{-m [num\_cols]} specify number of cols
                \item \texttt{-b [block\_size]} specify block size.
                \item \texttt{-t} display the time?
                \item \texttt{-r} seed with random value?
        \end{itemize}
        \section{Parallel Implementation}%
        Please see \texttt{gpu\_funcs.cu} for kernels. Use \texttt{-t} when running \texttt{./matrix} to display the CPU vs GPU times and speedups. 

        \texttt{vector\_reduction\_GPU} contains two internal kernel calls to \texttt{reduce0\_GPU} and \texttt{reduce1\_GPU}.  
        \section{Performance Improvement}%
\subsection{Rowsum Speedup vs CPU}%

        \begin{center}
 \begin{tabular}{||c | c | c | c | c ||}
 \hline
 Block size & n,m = 1000 & n,m = 5000 & n,m = 10000 & n,m = 25000 \\ [0.5ex] 
 \hline
 4 & 291 & 4826 & 20144 & 120456 \\
 \hline
 8 & 294 & 5040 & 19297 & 125733 \\
 \hline
 16 & 339 & 4830 & 19308 & 120458 \\
 \hline
 32 & 316 & 4839 & 19296 & 115626 \\
 \hline
 64 & 327 & 4831 & 20141 & 120557 \\
 \hline
 128 & 342 & 4835 & 19303 & 120450 \\
 \hline
 256 & 274 & 4824 & 20167 & 120445 \\
 \hline
 512 & 274 & 4838 & 19309 & 115690 \\
 \hline
 1024 & 274 & 5042 & 19322 & 120480 \\
 \hline

 \hline
\end{tabular}
\end{center}
We are clearly getting a massive speedup over the CPU when doing this kind of operation. The CPU is already quite good at doing rowsums in C due to contiguous data access (since C is row-major), however the incredible parallelism given by CUDA cannot be beaten. As is to be expected, the speedup increases as $m,n \to 25000$, since the proportion of time that data is being transmitted becomes a smaller fraction of the entire opration.

It seems that 16 is a close to optimal block size, since although it is not the fastest for $n, m = 10000$, it is consistently one of the fastest for each matrix dimension.
\pagebreak

\subsection{Column Sum Speedup vs CPU}%
\begin{center}
 \begin{tabular}{||c | c | c | c | c ||}
 \hline
 Block size & n,m = 1000 & n,m = 5000 & n,m = 10000 & n,m = 25000 \\ [0.5ex] 
 \hline
 4 & 1653 & 51842 & 279702 & 3697568 \\
 \hline
 8 & 1626 & 64977 & 276609 & 3694547 \\
 \hline
 16 & 2206 & 86626 & 280472 & 3886223 \\
 \hline
 32 & 1651 & 64723 & 277459 & 3758644 \\
 \hline
 64 & 1633 & 65055 & 281323 & 3772076 \\
 \hline
 128 & 1629 & 65184 & 275030 & 3739942 \\
 \hline
 256 & 1648 & 64428 & 275669 & 2954227 \\
 \hline
 512 & 2182 & 51512 & 277657 & 3715496 \\
 \hline
 256 & 1635 & 65210 & 277535 & 3033259 \\
 \hline
 \hline
\end{tabular}
\end{center}
We get an even better speedup for column sums when compared with the CPU. Not only is this due to the fact that the CPU is worse at column sums than rowsums, but also because we are able to do CUDA column sums an order of magnitude faster than CUDA rowsums (see \texttt{output/*.txt}). I'm not sure why this should be the case, if anything we would again assume that column-major C would make CUDA also slightly better at column sums than rowsums, or even almost the same if there was some fancy compiler trickery at work which made the memory access per thread more contiguous. 

        Once again 16 seems like a nice block size. We will use this for part 4.
\subsection{Reduction Speedup vs CPU}%
\begin{center}
 \begin{tabular}{||c | c | c | c | c ||}
 \hline
 Block size & n,m = 1000 & n,m = 5000 & n,m = 10000 & n,m = 25000 \\ [0.5ex] 
 \hline
 4 & 0.002 & 0.001 & 0.0005 & 0.0003 \\
 \hline
 8 & 0.002 & 0.002 & 0.0011 & 0.0005 \\
 \hline
 16 & 0.002 & 0.002 & 0.002 & 0.0008 \\
 \hline
 32 & 0.002 & 0.002 & 0.002 & 0.001 \\
 \hline
 64 & 0.002 & 0.002 & 0.002 & 0.001 \\
 \hline
 128 & 0.002 & 0.002 & 0.002 & 0.001 \\
 \hline
 256 & 0.002 & 0.002 & 0.002 & 0.001 \\
 \hline
 512 & 0.002 & 0.002 & 0.002 & 0.001 \\
 \hline
 1024 & 0.001 & 0.001 & 0.001 & 0.001 \\
 \hline
\end{tabular}
\end{center}
Reduction doesn't work well with CUDA! This is due to the fact that we need to combine all data into a single place, which is not easily achieved in CUDA, due to its low arithmetic intensity and also since it requires some global syncs (which I achieved using multiple kernel invocations). If we were able to use the shared memory then maybe the performance would be somewhat improved, but as it stands this is not a task that suits CUDA. 

If we were to get an efficient algorithm working using the shared memory and some other fancy optimisation tricks (as discussed in Mark Harris's CUDA reduction walkthrough---which incidentally only ever compares CUDA reductions to other CUDA reductions, not to CPU reduction speeds), then maybe we would be able to beat CPU reduction speeds. But at this point it must be asked: is it worthwhile to spend lots of time sweating over a fiddly algorithm that might only barely beat a CPU performance-wise? The answer is almost always \emph{no}. The best approach for an assignment like this would be to do the matrix operations on the GPU and then offload the reductions to the CPU. Hybrid solutions are always best.

\section{Double Precision Testing}%


\end{document}
