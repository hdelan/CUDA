\documentclass[a4paper, fleqn]{article}

\date{\today}
\author{Hugh Delaney}
\title{CUDA \\ Assignment 1}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools, geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage{xcolor}
\usepackage{listings}

\DeclarePairedDelimiterX{\inp}[2]{\langle}{\rangle}{#1, #2}

\setlength{\mathindent}{1cm}

% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{subfigure, pdfpages}
\usepackage{transparent}
\newcommand{\incfig}[1]{%
        \def\svgwidth{\columnwidth}
        \import{./figures/}{#1.pdf_tex}
}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}



\pdfsuppresswarningpagegroup=1

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}{Lemma}[theorem]

\renewcommand\qedsymbol{$\blacksquare$}

\begin{document}
\maketitle
The code for questions 1-3 can be found in the directory \texttt{single}, whereas the code for section 4 can be found in the directory \texttt{double}.  
        \section{CPU Calculation}%
        Please see \texttt{cpu\_funcs.cu} for implementation. \texttt{./matrix} accepts parameters:
        \begin{itemize}
                \item \texttt{-n [num\_rows]} specify number of rows
                \item \texttt{-m [num\_cols]} specify number of cols
                \item \texttt{-b [block\_size]} specify block size.
                \item \texttt{-t} display the time?
                \item \texttt{-r} seed with random value?
        \end{itemize}
        \section{Parallel Implementation}%
        Please see \texttt{gpu\_funcs.cu} for kernels. Use \texttt{-t} when running \texttt{./matrix} to display the CPU vs GPU times and speedups. 

        \texttt{vector\_reduction\_GPU} contains two internal kernel calls to \texttt{reduce0\_GPU} and \texttt{reduce1\_GPU}.  
        \section{Performance Improvement}%
\subsection{Rowsum Speedup vs CPU}%

        \begin{center}
 \begin{tabular}{||c | c | c | c | c ||}
 \hline
 Block size & n,m = 1000 & n,m = 5000 & n,m = 10000 & n,m = 25000 \\ [0.5ex] 
 \hline
 4 & 291 & 968 & 2023 & 4847 \\
 \hline
 8 & 339 & 972 & 2024 & 4650 \\
 \hline
 16 & 291 & 970 & 1937 & 5058 \\
 \hline
 32 & 316 & 1011 & 2023 & 4851 \\
 \hline
 64 & 315 & 969 & 2023 & 5056 \\
 \hline
 128 & 291 & 973 & 2023 & 5061 \\
 \hline
 256 & 299 & 1013 & 2114 & 4848 \\
 \hline
 512 & 274 & 1057 & 1942 & 4845 \\
 \hline
 1024 & 274 & 1015 & 1939 & 4846 \\
 \hline

 \hline
\end{tabular}
\end{center}
We are clearly getting a massive speedup over the CPU when doing this kind of operation. The CPU is already quite good at doing rowsums in C due to contiguous data access (since C is row-major), however the incredible parallelism given by CUDA cannot be beaten. As is to be expected, the speedup increases as $m,n \to 25000$, since the proportion of time that data is being transmitted becomes a smaller fraction of the entire operation.

It seems that anything from 16-256 could be chosen as optimal block size. There is some random variation in performance at times (perhaps due to other jobs on the machine) so I think any of these values could be called optimal. They're all pretty good!
\pagebreak

\subsection{Column Sum Speedup vs CPU}%
\begin{center}
 \begin{tabular}{||c | c | c | c | c ||}
 \hline
 Block size & n,m = 1000 & n,m = 5000 & n,m = 10000 & n,m = 25000 \\ [0.5ex] 
 \hline
 4 & 1757 & 11852 & 19568 & 65376 \\
 \hline
 8 & 2181 & 9368 & 3764 & 65684 \\
 \hline
 16 & 1610 & 11718 & 24468 & 65421 \\
 \hline
 32 & 1668 & 11755 & 24805 & 52240 \\
 \hline
 64 & 1630 & 9522 & 19810 & 65891 \\
 \hline
 128 & 1652 & 11825 & 24696 & 65435 \\
 \hline
 256 & 1652 & 11838 & 19945 & 66331 \\
 \hline
 512 & 1658 & 15682 & 24600 & 65529 \\
 \hline
 1024 & 1650 & 9444 & 24628 & 65865 \\
 \hline
 \hline
\end{tabular}
\end{center}
We get an even better speedup for column sums when compared with the CPU. Not only is this due to the fact that the CPU is worse at column sums than rowsums, but also because we are able to do CUDA column sums an order of magnitude faster than CUDA rowsums (see \texttt{output/*.txt}). I'm not sure why this should be the case, if anything we would again assume that column-major C would make CUDA also slightly better at column sums than rowsums, or even almost the same if there was some fancy compiler trickery at work which made the memory access per thread more contiguous. 

There seems to be more spread in terms of which block sizes perform well. The smaller block sizes seem to do well with smaller matrix dimensions, whereas bigger block sizes seem to do well for bigger matrix dimensions. For balance we will choose a middle block size (somewhat arbitrarily, since there must be some noise in the data that is causing any outlier values). We will choose 128 as our optimal block size for part 4.
\subsection{Reduction Speedup vs CPU}%
\begin{center}
 \begin{tabular}{||c | c | c | c | c ||}
 \hline
 Block size & n,m = 1000 & n,m = 5000 & n,m = 10000 & n,m = 25000 \\ [0.5ex] 
 \hline
 4 & 0.002 & 0.002 & 0.001 & 0.001 \\
 \hline
 8 & 0.002 & 0.002 & 0.002 & 0.001 \\
 \hline
 16 & 0.001 & 0.002 & 0.002 & 0.0008 \\
 \hline
 32 & 0.002 & 0.002 & 0.002 & 0.002 \\
 \hline
 64 & 0.002 & 0.002 & 0.002 & 0.002 \\
 \hline
 128 & 0.002 & 0.002 & 0.002 & 0.001 \\
 \hline
 256 & 0.002 & 0.002 & 0.002 & 0.002 \\
 \hline
 512 & 0.001 & 0.002 & 0.002 & 0.002 \\
 \hline
 1024 & 0.001 & 0.001 & 0.001 & 0.001 \\
 \hline
\end{tabular}
\end{center}
Reduction doesn't work well with CUDA! This is due to the fact that we need to combine all data into a single place, which is not easily achieved in CUDA, due to its low arithmetic intensity and also since it requires some global syncs (which I achieved using multiple kernel invocations). If we were able to use the shared memory then maybe the performance would be somewhat improved, but as it stands this is not a task that suits CUDA. 

If we were to get an efficient algorithm working using the shared memory and some other fancy optimisation tricks (as discussed in Mark Harris's CUDA reduction walkthrough---which incidentally only ever compares CUDA reductions to other CUDA reductions, not to CPU reduction speeds), then maybe we would be able to beat CPU reduction speeds. But at this point it must be asked: is it worthwhile to spend lots of time sweating over a fiddly algorithm that might only barely beat a CPU performance-wise? The answer is almost always \emph{no}. The best approach for an assignment like this would be to do the matrix operations on the GPU and then offload the reductions to the CPU. Hybrid solutions are always best.

\section{Double Precision Testing}%

Using blocksize 128 we can compare the performance between single and double precision. Please see \texttt{double/output/} for data. 


\begin{center}
 \begin{tabular}{||c | c | c | c | c ||}
 \hline
 Precision & n,m = 1000 & n,m = 5000 & n,m = 10000 & n,m = 25000 \\ [0.5ex] 
 \hline
 Single Precision & 0.002 & 0.002 & 0.001 & 0.001 \\
 \hline
 Double Precision & 0.002 & 0.002 & 0.002 & 0.001 \\
 \hline
 \hline
\end{tabular}
\end{center}

\end{document}
